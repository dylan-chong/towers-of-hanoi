# Report #

## What was done ##

Note: a '[x]' represents a completed task it is the markdown symbol for a
ticked tickbox

Minimum out of 30:
 - [x] (15) Creates the match table for the input pattern.
 - [x] (15) Performs correct KMP substring search with the pattern and text.

Core out of 30 (up to 70):
 - [x] (5) Finds the frequency histogram for an input text.
 - [x] (10) Creates the coding tree for an input text.
 - [x] (5) Correctly encodes input.
 - [x] (5) Correctly decodes input.
 - [x] (5) Answers questions 1 and 2.

Completion out of 30 (up to 90):
 - [ ] (10) Correctly performs Lempel Ziv compression.
 - [ ] (10) Correctly performs Lempel Ziv decompression.
 - [ ] (10) Answers questions 3 and 4.

Challenge out of 10 (up to 100, 5 spare marks):
 - [ ] (10) Implement Boyer-Moore string search OR a more complex coding
   algorithm, for example:
         - Adaptive Huffman coding
         - Arithmetic coding
 - [x] (5) Find a algorithm for question 5 so that `|Z| << 2*(|X| + |Y|)`.

-------------------------------------------------------------------------------

# Questions #

## Q1 ##

I wrote some benchmarks for comparison between BruteForce and KMP algorithms
See report-string-search-benchmark.png for comparisons - look at the 'median
duration field'.

KMP was mostly faster than brute force, which makes sense because the brute
force rechecks characters multiple times, especially if there are lots of
occurrences of the pattern's prefix.

The brute force search was faster for the 3rd test in the png file, where the
match was at the start, and the pattern was long. There is some overhead in
making the table, especially for a long pattern, so KMP would lag behind
during the checking of the pattern. This is the best case for the brute force
because no characters will have to be checked twice

## Q2 ##

Codes:

| Char   | Huffman Code           |
|--------|------------------------|
| \n     | 111010                 |
| \r     | 111001                 |
| space  | 110                    |
| !      | 1110000111             |
| "      | 11111010               |
| '      | 111000010              |
| (      | 1111101111111          |
| )      | 011000111000           |
| *      | 11111011010010         |
| ,      | 1111111                |
| -      | 100101001              |
| .      | 1110001                |
| /      | 011000111001010111110  |
| 0      | 111110110100001        |
| 1      | 11111011010001         |
| 2      | 111110110100000        |
| 3      | 0110001110010111       |
| 4      | 01100011100101010      |
| 5      | 0110001110010100       |
| 6      | 0110001110010110       |
| 7      | 01100011100111110      |
| 8      | 01100011100100         |
| 9      | 01100011100111101      |
| :      | 111000001001           |
| ;      | 111110110101           |
| =      | 011000111001010111111  |
| ?      | 1001010100             |
| A      | 011000110              |
| B      | 1110000001             |
| C      | 01100010000            |
| D      | 11111011000            |
| E      | 01100010001            |
| F      | 11100000101            |
| G      | 111110111101           |
| H      | 1110000011             |
| I      | 100101011              |
| J      | 11111011010011         |
| K      | 111110111100           |
| L      | 1111101111110          |
| M      | 1001010101             |
| N      | 1110000000             |
| O      | 01100011101            |
| P      | 011000101              |
| Q      | 01100011100111111      |
| R      | 11111011011            |
| S      | 0110001111             |
| T      | 100101000              |
| U      | 01100011100110         |
| V      | 111000001000           |
| W      | 0110001001             |
| X      | 01100011100111100      |
| Y      | 111110111110           |
| Z      | 011000111001110        |
| à      | 0110001110010101110    |
| a      | 1000                   |
| b      | 1111100                |
| c      | 101111                 |
| d      | 10110                  |
| ä      | 0110001110010101111010 |
| e      | 000                    |
| f      | 100110                 |
| g      | 100100                 |
| h      | 0011                   |
| é      | 0110001110010101111011 |
| i      | 0100                   |
| j      | 11111011001            |
| ê      | 011000111001010110     |
| k      | 0110000                |
| l      | 01101                  |
| m      | 101110                 |
| n      | 0101                   |
| o      | 0111                   |
| p      | 1111110                |
| q      | 11111011101            |
| r      | 11110                  |
| s      | 0010                   |
| t      | 1010                   |
| u      | 111011                 |
| v      | 1001011                |
| w      | 100111                 |
| x      | 1110000110             |
| y      | 011001                 |
| z      | 11111011100            |
| \uFEFF | 011000111001010111100  |

input length:  `3258246 bytes`
output length: `1848598 bytes`

## Q3 ##

| File              | Input bytes | Output bytes | In/Out (reduction factor) |
|-------------------|-------------|--------------|---------------------------|
| war_and_peace.txt | 3258246     | 1848598      | 1.762549781               |
| taisho.txt        | 3649944     | 1542656      | 2.366012902               |
| pi.txt            | 1010003     | 443632       | 2.2766685                 |

All of the files did very well, resulting in files roughly 1/2 the size,
excluding the data required to store the codes.

This is most likely because each character required 8 bits, because of UTF-8.
In the Pi example, there are only 12 different characters (`[\d\.\n]`), so only
`ceil(log_2(12)) == 4` bits are required. This results in a file roughly half
the size.

I suspect the Taisho was the best because certain characters are used very very
frequently, and many very infrequently, which makes the Huffman algorithm be
more effective. Taisho is probably the worst if you include the coding data -
there was over 8000 different characters, which would take up quite a bit of
space to store.

Perhaps the war_and_peace.txt got less than 2x reduction in size because there
are lots of characters that are all used at least somewhat frequently. So many
times, characters require more than 4 bits to encode.

## Q4 ##

### Benchmarks for different window sizes for each file ###

Notes:
- out/in ratio : Smaller value means a smaller output file size
- window size  : The number of characters that can the algoritm can look back

File : ./data/apollo.txt
- window size            : 255
  - input length         : 6815380
  - output length        : 15772951
  - out/in ratio         : 2.314317
  - compression duration : 8.548000 s
- window size            : 1023
  - input length         : 6815380
  - output length        : 14153114
  - out/in ratio         : 2.076643
  - compression duration : 11.572000 s
- window size            : 4095
  - input length         : 6815380
  - output length        : 13432704
  - out/in ratio         : 1.970940
  - compression duration : 29.685000 s

File : ./data/lenna.txt
- window size            : 255
  - input length         : 306296
  - output length        : 894516
  - out/in ratio         : 2.920430
  - compression duration : 0.322000 s
- window size            : 1023
  - input length         : 306296
  - output length        : 771348
  - out/in ratio         : 2.518309
  - compression duration : 0.560000 s
- window size            : 4095
  - input length         : 306296
  - output length        : 723513
  - out/in ratio         : 2.362137
  - compression duration : 1.482000 s

File : ./data/pi.txt
- window size            : 255
  - input length         : 1010003
  - output length        : 2828319
  - out/in ratio         : 2.800308
  - compression duration : 1.055000 s
- window size            : 1023
  - input length         : 1010003
  - output length        : 2469979
  - out/in ratio         : 2.445516
  - compression duration : 2.019000 s
- window size            : 4095
  - input length         : 1010003
  - output length        : 2316745
  - out/in ratio         : 2.293800
  - compression duration : 5.627000 s

File : ./data/taisho.txt
- window size            : 255
  - input length         : 1382354
  - output length        : 5621299
  - out/in ratio         : 4.066469
  - compression duration : 1.828000 s
- window size            : 1023
  - input length         : 1382354
  - output length        : 5076364
  - out/in ratio         : 3.672261
  - compression duration : 2.982000 s
- window size            : 4095
  - input length         : 1382354
  - output length        : 4806350
  - out/in ratio         : 3.476931
  - compression duration : 7.528000 s

File : ./data/war_and_peace.txt
- window size            : 255
  - input length         : 3196214
  - output length        : 8779661
  - out/in ratio         : 2.746894
  - compression duration : 3.262000 s
- window size            : 1023
  - input length         : 3196214
  - output length        : 6987938
  - out/in ratio         : 2.186317
  - compression duration : 5.003000 s
- window size            : 4095
  - input length         : 3196214
  - output length        : 6090005
  - out/in ratio         : 1.905381
  - compression duration : 12.607000 s

As can be seen, as the window size increased, the file sized reduced, by a
noticeable, but not signifcant amount. The size reduction decreased because
better matches can be found due to the larger window (more matches, and larger
ones).

It did dramatically increase the time required to compress the data though -
because there are more steps to do to find the longest match within the window.

## Q5 ##

### Compress using Huffman then LempelZiv ###

File : ./data/war_and_peace.txt
- window size            : 255
  - input length         : 3196214
  - output length        : 34443284
  - out/in ratio         : 10.776276
  - compression duration : 4.550000 s
- window size            : 1023
  - input length         : 3196214
  - output length        : 28598245
  - out/in ratio         : 8.947538
  - compression duration : 6.272000 s
- window size            : 4095
  - input length         : 3196214
  - output length        : 25035419
  - out/in ratio         : 7.832836
  - compression duration : 14.466000 s

### Same test as above but just with LempelZiv ##

File : ./data/war_and_peace.txt
- window size            : 255
  - input length         : 3196214
  - output length        : 8779661
  - out/in ratio         : 2.746894
  - compression duration : 4.911000 s
- window size            : 1023
  - input length         : 3196214
  - output length        : 6987938
  - out/in ratio         : 2.186317
  - compression duration : 6.603000 s
- window size            : 4095
  - input length         : 3196214
  - output length        : 6090005
  - out/in ratio         : 1.905381
  - compression duration : 15.406000 s

The file size is much larger - slightly less than 8x larger than using
LempelZiv alone. This is probably because the Huffman codes are wasteful - they
represent 1's and 0's using 8 bits each, so Huffman takes up 8x the space than
is required. 

It is slightly less than 8x probably because with binary there are only 2
characters, so there are going to be many repeated patterns to help reduce file
size, which LempelZiv shortens.

## Q5 again (i.e. Q6) ##

let `S` be the smallest of X and Y, and `B` be the bigger

let `L` be 32-bit binary string representing the length of `S` (limits smallest
file to 4GB)

let `W` == 1 iff `|X| < |Y|` (so we know to switch X and Y around later) `Z`
will be the string literal `${W}${L}${S}${B}`, where `${stuff}` evaluates
`stuff` (like string interpolation)

`|Z|` is equal to `|X| + |Y| + 1 + ${|L|}`, where `|L|` is 32. This is smaller
than `2*(|X| + |Y|)` when `|X| + |Y| > 33`.
